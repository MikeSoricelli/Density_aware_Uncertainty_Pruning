{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MikeSoricelli/Density_aware_Uncertainty_Pruning/blob/main/density_based_uncertainty_pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yAH--5QhKaQ",
        "outputId": "559bfd1b-9b24-4e24-8848-a94dfb00ec96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting normflows\n",
            "  Downloading normflows-1.7.3.tar.gz (65 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/65.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from normflows) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from normflows) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->normflows) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->normflows) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->normflows) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->normflows) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->normflows) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->normflows)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->normflows)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->normflows)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->normflows)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->normflows)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->normflows)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->normflows)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->normflows)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->normflows)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->normflows) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->normflows) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->normflows) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->normflows)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->normflows) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->normflows) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->normflows) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->normflows) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: normflows\n",
            "  Building wheel for normflows (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for normflows: filename=normflows-1.7.3-py2.py3-none-any.whl size=87247 sha256=a108d1ced93fd1fbf273f09cde8cd9e1095fde049aa4007e1329741642c86c2f\n",
            "  Stored in directory: /root/.cache/pip/wheels/60/5c/bf/9df82daf4999bb59abf8263753184465d1adfe2bed2b9c5a77\n",
            "Successfully built normflows\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, normflows\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed normflows-1.7.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "pip install normflows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtiMXZpgnc5X",
        "outputId": "6d3ee54a-cc5e-479d-f096-bb20b4ea5f1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipdb\n",
            "  Downloading ipdb-0.13.13-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: ipython>=7.31.1 in /usr/local/lib/python3.11/dist-packages (from ipdb) (7.34.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.31.1->ipdb)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.31.1->ipdb) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=7.31.1->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.31.1->ipdb) (0.2.13)\n",
            "Downloading ipdb-0.13.13-py3-none-any.whl (12 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi, ipdb\n",
            "Successfully installed ipdb-0.13.13 jedi-0.19.2\n"
          ]
        }
      ],
      "source": [
        "!pip install ipdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNDputdo1xAA",
        "outputId": "b9098701-abc0-443d-9c40-21580832c224"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#import libraries\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import models, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import normflows as nf\n",
        "import ipdb\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2KNp6tzAf1X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class NoisyCIFAR10(Dataset):\n",
        "    def __init__(self, root, train=True, download=True, noise_std=0.35, transform=None):\n",
        "        # Load CIFAR-10 dataset\n",
        "        self.base_dataset = datasets.CIFAR10(root=root, train=train, download=download, transform=transform)\n",
        "        self.noise_std = noise_std\n",
        "        self.transform = transform\n",
        "\n",
        "        # Decide which indices will be noisy (random half)\n",
        "        self.is_noisy = np.zeros(len(self.base_dataset), dtype=bool)\n",
        "        noisy_indices = np.random.choice(len(self.base_dataset), size=len(self.base_dataset)//2, replace=False)\n",
        "        self.is_noisy[noisy_indices] = True\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.base_dataset[idx]\n",
        "        # img is a PIL Image if no transform, or a Tensor if transform includes ToTensor\n",
        "        if self.is_noisy[idx]:\n",
        "            if isinstance(img, torch.Tensor):\n",
        "                noise = torch.randn_like(img) * self.noise_std\n",
        "                img = torch.clamp(img + noise, 0., 1.)\n",
        "            else:\n",
        "                # If still PIL, convert to tensor, add noise, then back to PIL if needed\n",
        "                img = transforms.ToTensor()(img)\n",
        "                noise = torch.randn_like(img) * self.noise_std\n",
        "                img = torch.clamp(img + noise, 0., 1.)\n",
        "                if self.transform is not None and not any(isinstance(t, transforms.ToTensor) for t in self.transform.transforms):\n",
        "                    img = transforms.ToPILImage()(img)\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset)\n",
        "\n",
        "class NoisyCIFAR100(Dataset):\n",
        "    def __init__(self, root, train=True, download=True, noise_std=0.35, transform=None):\n",
        "        # Load CIFAR-10 dataset\n",
        "        self.base_dataset = datasets.CIFAR100(root=root, train=train, download=download, transform=transform)\n",
        "        self.noise_std = noise_std\n",
        "        self.transform = transform\n",
        "\n",
        "        # Decide which indices will be noisy (random half)\n",
        "        self.is_noisy = np.zeros(len(self.base_dataset), dtype=bool)\n",
        "        noisy_indices = np.random.choice(len(self.base_dataset), size=len(self.base_dataset)//2, replace=False)\n",
        "        self.is_noisy[noisy_indices] = True\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.base_dataset[idx]\n",
        "        # img is a PIL Image if no transform, or a Tensor if transform includes ToTensor\n",
        "        if self.is_noisy[idx]:\n",
        "            if isinstance(img, torch.Tensor):\n",
        "                noise = torch.randn_like(img) * self.noise_std\n",
        "                img = torch.clamp(img + noise, 0., 1.)\n",
        "            else:\n",
        "                # If still PIL, convert to tensor, add noise, then back to PIL if needed\n",
        "                img = transforms.ToTensor()(img)\n",
        "                noise = torch.randn_like(img) * self.noise_std\n",
        "                img = torch.clamp(img + noise, 0., 1.)\n",
        "                if self.transform is not None and not any(isinstance(t, transforms.ToTensor) for t in self.transform.transforms):\n",
        "                    img = transforms.ToPILImage()(img)\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3hhbvc9Dezh",
        "outputId": "26b004a8-a2bb-4f3f-eced-47f32fd86c59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61pebQ8xYSRP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleResidualBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.relu1(out)\n",
        "        out = self.conv2(out)\n",
        "        return self.relu2(out) + x # ReLU can be applied before or after adding the input\n",
        "\n",
        "def conv_block(in_channels, out_channels, pool=False):\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "              nn.BatchNorm2d(out_channels),\n",
        "              nn.ReLU(inplace=True)]\n",
        "    if pool: layers.append(nn.MaxPool2d(2))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class ResNet9(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes=10, input_size=32):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = conv_block(in_channels, 64)\n",
        "        self.conv2 = conv_block(64, 128, pool=True)\n",
        "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
        "\n",
        "        self.conv3 = conv_block(128, 256, pool=True)\n",
        "        self.conv4 = conv_block(256, 512, pool=True)\n",
        "        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
        "\n",
        "        self.classifier = nn.Linear(512, num_classes)\n",
        "        self.fc1 = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n",
        "                                        nn.Flatten(),\n",
        "                                        )\n",
        "\n",
        "    def forward(self, xb):\n",
        "        out = self.conv1(xb)\n",
        "        out = self.conv2(out)\n",
        "        out = self.res1(out) + out\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.res2(out) + out\n",
        "        out = self.fc1(out)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kl2YCPdOQT-U"
      },
      "outputs": [],
      "source": [
        "def calculate_variance(predictions, start_window, end_window):\n",
        "    predictions = predictions[start_window:end_window]\n",
        "    predictions = np.array(predictions)\n",
        "    variance_predictions = np.var(predictions, axis=0)\n",
        "    return np.mean(variance_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjsbMV6-1XkE"
      },
      "outputs": [],
      "source": [
        "## dynUnc implementation https://arxiv.org/pdf/2306.05175\n",
        "\n",
        "def compute_dyn_unc(probabilities, window_size, total_epochs):\n",
        "    uncertainties = [0] * len(trainset)\n",
        "    for epoch in range(total_epochs):\n",
        "        if epoch >= window_size:\n",
        "            # window_predictions = get_predictions(epoch-window_size, epoch)\n",
        "            # uncertainties.append(calculate_variance(window_predictions))\n",
        "            # predictions = probabilities[epoch-window_size:epoch]\n",
        "\n",
        "            #for each data point calculate the uncertainty at this position and append it to the uncertainbty over all of training\n",
        "            for i in range(len(trainset)):\n",
        "                uncertainties[i] += calculate_variance(probabilties[i], epoch - window_size, epoch)\n",
        "        else:\n",
        "            uncertainties.append(0.0)\n",
        "        print(epoch)\n",
        "    uncertainties = np.array(uncertainties)\n",
        "    return uncertainties / (total_epochs - window_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0v4ydCiNZdg"
      },
      "outputs": [],
      "source": [
        "#prune dataset\n",
        "\n",
        "def prune_dataset(dataset, ratio, model, uncertainties, probabilities, beta):\n",
        "\n",
        "    #normalize unceratatinties and probabiliteis\n",
        "\n",
        "    uncertainties = np.array(uncertainties)\n",
        "    probabilities = np.array(probabilities)\n",
        "\n",
        "    #manually get min for probabitlies due to underflow\n",
        "    prob_min = 0\n",
        "    for i in range(len(probabilities)):\n",
        "        if probabilities[i] != float('-inf') and probabilities[i] < prob_min:\n",
        "            prob_min = probabilities[i]\n",
        "\n",
        "\n",
        "    for i in range(len(probabilities)):\n",
        "        if probabilities[i] == float('-inf'):\n",
        "            probabilities[i] = prob_min\n",
        "\n",
        "\n",
        "    uncertainties = (uncertainties - np.min(uncertainties)) / (np.max(uncertainties) - np.min(uncertainties))\n",
        "    probabilities = (probabilities - np.min(probabilities)) / (np.max(probabilities) - np.min(probabilities))\n",
        "\n",
        "\n",
        "    # Balance standard deviations\n",
        "    prob_std = probabilities.std()\n",
        "    uncert_std = uncertainties.std()\n",
        "    uncertainties = uncertainties * (prob_std / uncert_std)\n",
        "\n",
        "    scores = []\n",
        "    for i in range(len(dataset)):\n",
        "        scores.append((beta * uncertainties[i]) + ((1 - beta) * (probabilities[i])))\n",
        "    threshold = np.percentile(scores, 100*(1-ratio))\n",
        "\n",
        "    indices = []\n",
        "\n",
        "    for i in range(len(scores)):\n",
        "        if scores[i] >= threshold:\n",
        "            indices.append(i)\n",
        "\n",
        "    return [dataset[i] for i in indices], indices\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3jpT3pm1syC"
      },
      "source": [
        "#CIFAR 100 Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSrhwLCcg_sD",
        "outputId": "e17922e2-447d-4ab5-942f-52ef391f4766"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_tensor.py:902: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.\n",
            "LU, pivots = torch.lu(A, compute_pivots)\n",
            "should be replaced with\n",
            "LU, pivots = torch.linalg.lu_factor(A, compute_pivots)\n",
            "and\n",
            "LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)\n",
            "should be replaced with\n",
            "LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at /pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2053.)\n",
            "  LU, pivots, infos = torch._lu_with_info(\n"
          ]
        }
      ],
      "source": [
        "#flow model definition\n",
        "\n",
        "# Define flows\n",
        "L = 3\n",
        "K = 16\n",
        "torch.manual_seed(0)\n",
        "\n",
        "input_shape = (3, 32, 32)\n",
        "n_dims = np.prod(input_shape)\n",
        "channels = 3\n",
        "hidden_channels = 256\n",
        "split_mode = 'channel'\n",
        "scale = True\n",
        "num_classes = 100\n",
        "\n",
        "# Set up flows, distributions and merge operations\n",
        "q0 = []\n",
        "merges = []\n",
        "flows = []\n",
        "for i in range(L):\n",
        "    flows_ = []\n",
        "    for j in range(K):\n",
        "        flows_ += [nf.flows.GlowBlock(channels * 2 ** (L + 1 - i), hidden_channels,\n",
        "                                     split_mode=split_mode, scale=scale)]\n",
        "    flows_ += [nf.flows.Squeeze()]\n",
        "    flows += [flows_]\n",
        "    if i > 0:\n",
        "        merges += [nf.flows.Merge()]\n",
        "        latent_shape = (input_shape[0] * 2 ** (L - i), input_shape[1] // 2 ** (L - i),\n",
        "                        input_shape[2] // 2 ** (L - i))\n",
        "    else:\n",
        "        latent_shape = (input_shape[0] * 2 ** (L + 1), input_shape[1] // 2 ** L,\n",
        "                        input_shape[2] // 2 ** L)\n",
        "    q0 += [nf.distributions.ClassCondDiagGaussian(latent_shape, num_classes)]\n",
        "\n",
        "\n",
        "# Construct flow model with the multiscale architecture\n",
        "flow_model = nf.MultiscaleFlow(q0, flows, merges)\n",
        "\n",
        "# Move model on GPU if available\n",
        "enable_cuda = True\n",
        "device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n",
        "flow_model = flow_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUL3ahmT2QZK",
        "outputId": "83afdb64-3e75-4693-a9e1-2cb44fc6f55c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:13<00:00, 12.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "#setup up cifar 100 dataset\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2762)),\n",
        "])\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2762))])\n",
        "\n",
        "trainset = NoisyCIFAR100(train=True, download=True, root='./data', noise_std=0.25, transform=transform_train)\n",
        "testset = datasets.CIFAR100(train=False, download=True, root='./data', transform=transform)\n",
        "\n",
        "trainLoader = DataLoader(trainset, batch_size=256, num_workers=4, shuffle=False)\n",
        "testLoader = DataLoader(testset, batch_size=256, num_workers=4, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVpVb91H2rwb",
        "outputId": "e179f81c-ffbc-474c-d72e-cf46343c4ec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch = 0, test accuracy = 0.16210000216960907\n",
            "epoch = 1, test accuracy = 0.23589999973773956\n",
            "epoch = 2, test accuracy = 0.3046000003814697\n",
            "epoch = 3, test accuracy = 0.3741999864578247\n",
            "epoch = 4, test accuracy = 0.4153999984264374\n",
            "epoch = 5, test accuracy = 0.4486999809741974\n",
            "epoch = 6, test accuracy = 0.4339999854564667\n",
            "epoch = 7, test accuracy = 0.48339998722076416\n",
            "epoch = 8, test accuracy = 0.4885999858379364\n",
            "epoch = 9, test accuracy = 0.491599977016449\n",
            "epoch = 10, test accuracy = 0.5194000005722046\n",
            "epoch = 11, test accuracy = 0.5277000069618225\n",
            "epoch = 12, test accuracy = 0.5317000150680542\n",
            "epoch = 13, test accuracy = 0.5525999665260315\n",
            "epoch = 14, test accuracy = 0.5425999760627747\n",
            "epoch = 15, test accuracy = 0.538599967956543\n",
            "epoch = 16, test accuracy = 0.5555999875068665\n",
            "epoch = 17, test accuracy = 0.5565999746322632\n",
            "epoch = 18, test accuracy = 0.5537999868392944\n",
            "epoch = 19, test accuracy = 0.5500999689102173\n",
            "epoch = 20, test accuracy = 0.5758000016212463\n",
            "epoch = 21, test accuracy = 0.5615999698638916\n",
            "epoch = 22, test accuracy = 0.5672999620437622\n",
            "epoch = 23, test accuracy = 0.571399986743927\n",
            "epoch = 24, test accuracy = 0.5684999823570251\n",
            "epoch = 25, test accuracy = 0.5662999749183655\n",
            "epoch = 26, test accuracy = 0.5658999681472778\n",
            "epoch = 27, test accuracy = 0.5777999758720398\n",
            "epoch = 28, test accuracy = 0.5708000063896179\n",
            "epoch = 29, test accuracy = 0.5758000016212463\n",
            "epoch = 30, test accuracy = 0.5679000020027161\n",
            "epoch = 31, test accuracy = 0.576200008392334\n",
            "epoch = 32, test accuracy = 0.5715000033378601\n",
            "epoch = 33, test accuracy = 0.5647000074386597\n",
            "epoch = 34, test accuracy = 0.5755000114440918\n",
            "epoch = 35, test accuracy = 0.5997999906539917\n",
            "epoch = 36, test accuracy = 0.5753999948501587\n",
            "epoch = 37, test accuracy = 0.5824999809265137\n",
            "epoch = 38, test accuracy = 0.5823999643325806\n",
            "epoch = 39, test accuracy = 0.5704999566078186\n",
            "epoch = 40, test accuracy = 0.5972999930381775\n",
            "epoch = 41, test accuracy = 0.585099995136261\n",
            "epoch = 42, test accuracy = 0.5813999772071838\n",
            "epoch = 43, test accuracy = 0.5914999842643738\n",
            "epoch = 44, test accuracy = 0.6020999550819397\n",
            "epoch = 45, test accuracy = 0.6037999987602234\n",
            "epoch = 46, test accuracy = 0.5884999632835388\n",
            "epoch = 47, test accuracy = 0.5916999578475952\n",
            "epoch = 48, test accuracy = 0.5938999652862549\n",
            "epoch = 49, test accuracy = 0.5831999778747559\n"
          ]
        }
      ],
      "source": [
        "#intilaize model and run trianig loop, stroe probablities at eahc epoch for teh dataset\n",
        "\n",
        "#data structure for trainng probabilties\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ResNet9(3, 100, 32).to(device)\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "epoch = 50\n",
        "\n",
        "\n",
        "probabilties = [[] for i in range(len(trainset))]\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(epoch):\n",
        "    model.train()\n",
        "    idx = 0\n",
        "    for batch_idx, (inputs, labels) in enumerate(trainLoader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track probabilities using absolute indices\n",
        "        start_idx = batch_idx * trainLoader.batch_size\n",
        "        for i, output in enumerate(outputs.detach().cpu().numpy()):\n",
        "            probabilties[start_idx + i].append(output)  # Matches dataset order\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for image, label in testLoader:\n",
        "        image = image.to(device)\n",
        "        label = label.to(device)\n",
        "        output = model(image)\n",
        "        predictions = torch.argmax(output, dim=1)\n",
        "        correct += torch.sum((predictions == label))\n",
        "    print(f'epoch = {epoch}, test accuracy = {correct/len(testset)}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcszL99CHlOs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmdDko0QolwR",
        "outputId": "b957610f-f440-446e-f565-066cc588bbe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n"
          ]
        }
      ],
      "source": [
        "uncertainties = compute_dyn_unc(probabilties, 10, 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ76c3DpcJyM",
        "outputId": "15f5843f-7ed8-4d9c-e0a3-c59d578b2dd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "#load flow model\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 1. Instantiate your NICE model\n",
        "# Construct flow model with the multiscale architecture\n",
        "# 2. Load the state dictionary into the model\n",
        "state_dict = torch.load('/content/drive/MyDrive/flowModels/Glow_CIFAR100_Clean.pth', map_location=torch.device('cuda'))\n",
        "flow_model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4PU_AYOhwXW"
      },
      "outputs": [],
      "source": [
        "#generate log probabilites of each dta point:\n",
        "\n",
        "probabilities = []\n",
        "\n",
        "max_float32 = np.finfo(np.float32).max\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for image, targets in trainLoader:\n",
        "        image = image.to(device)\n",
        "        log_prob = flow_model.log_prob(image, targets.to(device))\n",
        "        prob = torch.exp(log_prob)\n",
        "        print(targets)\n",
        "        for i in range(len(image)):\n",
        "            print(log_prob[i].item())\n",
        "            probabilities.append(log_prob[i].item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Li8tqtRkFjIj",
        "outputId": "9c72ab1e-3187-4759-be61-8199540d8079"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.2170527 , 0.29429865, 0.1799405 , ..., 0.        , 0.        ,\n",
              "       0.        ])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(uncertainties - np.min(uncertainties)) / (np.max(uncertainties) - np.min(uncertainties))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DL-GWmiUG-a3",
        "outputId": "03d221fd-862f-4c88-84c7-0f5f034981b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-3ee390340665>:1: RuntimeWarning: invalid value encountered in subtract\n",
            "  values = (probabilities - np.min(probabilities)) / (np.max(probabilities) - np.min(probabilities))\n",
            "<ipython-input-14-3ee390340665>:1: RuntimeWarning: invalid value encountered in divide\n",
            "  values = (probabilities - np.min(probabilities)) / (np.max(probabilities) - np.min(probabilities))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "np.float64(nan)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "values = (probabilities - np.min(probabilities)) / (np.max(probabilities) - np.min(probabilities))\n",
        "\n",
        "values.min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vyr9zwCCh08R"
      },
      "outputs": [],
      "source": [
        "pruned_dataset = prune_dataset(trainset, 0.5, model, uncertainties, probabilities, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qwgDlSh3SZe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAVwOfcWj9EX",
        "outputId": "e06c87dd-de76-4729-d368-8c720d7ee380"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(pruned_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNLIa73Bm_TP"
      },
      "outputs": [],
      "source": [
        "prune_trainLoader = DataLoader(pruned_dataset, batch_size=256, num_workers=4, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyLOAwyMnF1e",
        "outputId": "ee1af1d1-fad7-488d-c5c9-95b4e435bbde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.4701, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "for inputs, labels in testLoader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    outputs = model(inputs)\n",
        "    predictions = torch.argmax(outputs, dim=1)\n",
        "    correct += torch.sum((predictions == labels))\n",
        "print(correct/len(testset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1cj2dxhl2U0",
        "outputId": "bab65124-2a6f-4e32-ea11-d086d05685db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size: 0.5\n",
            "ratio 1\n",
            "max accuracy: 0.5094999670982361\n",
            "max accuracy: 0.5121999979019165\n",
            "[tensor(9.2641, device='cuda:0'), 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "ratio 0.5\n",
            "max accuracy: 0.5288999676704407\n",
            "max accuracy: 0.5223999619483948\n",
            "[tensor(9.2942, device='cuda:0'), 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "size: 0.25\n",
            "ratio 1\n",
            "max accuracy: 0.44119998812675476\n",
            "max accuracy: 0.43529999256134033\n",
            "[tensor(7.6588, device='cuda:0'), 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "ratio 0.5\n",
            "max accuracy: 0.4527999758720398\n",
            "max accuracy: 0.445499986410141\n",
            "[tensor(7.6458, device='cuda:0'), 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ],
      "source": [
        "#train model on pruned dataset\n",
        "\n",
        "ratios = [1, 0.5]\n",
        "\n",
        "sizes = [0.5, 0.25]\n",
        "\n",
        "for k in sizes:\n",
        "\n",
        "  print(f'size: {k}')\n",
        "\n",
        "\n",
        "  for i in ratios:\n",
        "\n",
        "      print(f'ratio {i}')\n",
        "\n",
        "      accuracies = [0.0] * 9\n",
        "      ratio_idx = 0\n",
        "      for j in range(2):\n",
        "\n",
        "          max_accuracy = 0\n",
        "\n",
        "\n",
        "          pruned_dataset = prune_dataset(trainset, k, model, uncertainties, probabilities, i)\n",
        "\n",
        "          prune_trainLoader = DataLoader(pruned_dataset, batch_size=256, num_workers=4, shuffle=False)\n",
        "\n",
        "          pruned_model = ResNet9(3, 100, 32)\n",
        "\n",
        "          pruned_model.to(device)\n",
        "\n",
        "          epoch = 50\n",
        "\n",
        "          optimizer = optim.Adam(pruned_model.parameters(), lr=0.001)\n",
        "          criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "          for epoch in range(epoch):\n",
        "              idx = 0\n",
        "              for batch_idx, (inputs, labels) in enumerate(prune_trainLoader):\n",
        "                  inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                  # Forward pass\n",
        "                  outputs = pruned_model(inputs)\n",
        "                  loss = criterion(outputs, labels)\n",
        "\n",
        "                  # Backward pass\n",
        "                  optimizer.zero_grad()\n",
        "                  loss.backward()\n",
        "                  optimizer.step()\n",
        "\n",
        "              correct = 0\n",
        "              for inputs, labels in testLoader:\n",
        "                  inputs, labels = inputs.to(device), labels.to(device)\n",
        "                  outputs = pruned_model(inputs)\n",
        "                  predictions = torch.argmax(outputs, dim=1)\n",
        "                  correct += torch.sum((predictions == labels))\n",
        "              # print(f'epoch:{epoch}, accuracy:{correct/len(testset)}')\n",
        "              if correct/len(testset) > max_accuracy:\n",
        "                  max_accuracy = correct/len(testset)\n",
        "\n",
        "              accuracies[ratio_idx] += correct/len(testset)\n",
        "\n",
        "          print(f'max accuracy: {max_accuracy}')\n",
        "\n",
        "      accuracies[ratio_idx] /= 5\n",
        "      ratio_idx += 1\n",
        "      print(accuracies)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijEjh1asWz-D"
      },
      "source": [
        "size: 0.75\n",
        "\n",
        "just dynunc\n",
        "epoch:99, accuracy:0.3545999825000763\n",
        "epoch:99, accuracy:0.3580999970436096\n",
        "epoch:99, accuracy:0.3580999970436096\n",
        "epoch:99, accuracy:0.3612000048160553\n",
        "\n",
        "ratio 0.25\n",
        "\n",
        "epoch:99, accuracy:0.33390000462532043\n",
        "epoch:99, accuracy:0.3407999873161316\n",
        "epoch:99, accuracy:0.3303000032901764\n",
        "\n",
        "ratio 0.5\n",
        "epoch:99, accuracy:0.343999981880188\n",
        "epoch:99, accuracy:0.336899995803833\n",
        "\n",
        "\n",
        "ratio 0.75\n",
        "\n",
        "\n",
        "size 0.25:\n",
        "\n",
        "ratio 1:\n",
        "\n",
        "epoch:99, accuracy:0.24169999361038208\n",
        "epoch:99, accuracy:0.2410999983549118\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3p6TV9cKMxCD"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weneefN5LnWP"
      },
      "source": [
        "epoch:0, accuracy:0.18400000035762787\n",
        "epoch:10, accuracy:0.29819998145103455\n",
        "epoch:20, accuracy:0.33709999918937683\n",
        "epoch:30, accuracy:0.346699982881546\n",
        "epoch:40, accuracy:0.36319997906684875\n",
        "epoch:50, accuracy:0.36409997940063477\n",
        "epoch:60, accuracy:0.36629998683929443\n",
        "epoch:70, accuracy:0.36550000309944153\n",
        "epoch:80, accuracy:0.3651999831199646\n",
        "epoch:90, accuracy:0.36569997668266296"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WsAXFtrMyFU"
      },
      "source": [
        "#CIFAR10 Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBDvRxzvM26_"
      },
      "outputs": [],
      "source": [
        "#flow model definition\n",
        "\n",
        "# Define flows\n",
        "L = 3\n",
        "K = 16\n",
        "torch.manual_seed(0)\n",
        "\n",
        "input_shape = (3, 32, 32)\n",
        "n_dims = np.prod(input_shape)\n",
        "channels = 3\n",
        "hidden_channels = 256\n",
        "split_mode = 'channel'\n",
        "scale = True\n",
        "num_classes = 10\n",
        "\n",
        "# Set up flows, distributions and merge operations\n",
        "q0 = []\n",
        "merges = []\n",
        "flows = []\n",
        "for i in range(L):\n",
        "    flows_ = []\n",
        "    for j in range(K):\n",
        "        flows_ += [nf.flows.GlowBlock(channels * 2 ** (L + 1 - i), hidden_channels,\n",
        "                                     split_mode=split_mode, scale=scale)]\n",
        "    flows_ += [nf.flows.Squeeze()]\n",
        "    flows += [flows_]\n",
        "    if i > 0:\n",
        "        merges += [nf.flows.Merge()]\n",
        "        latent_shape = (input_shape[0] * 2 ** (L - i), input_shape[1] // 2 ** (L - i),\n",
        "                        input_shape[2] // 2 ** (L - i))\n",
        "    else:\n",
        "        latent_shape = (input_shape[0] * 2 ** (L + 1), input_shape[1] // 2 ** L,\n",
        "                        input_shape[2] // 2 ** L)\n",
        "    q0 += [nf.distributions.ClassCondDiagGaussian(latent_shape, num_classes)]\n",
        "\n",
        "\n",
        "# Construct flow model with the multiscale architecture\n",
        "flow_model = nf.MultiscaleFlow(q0, flows, merges)\n",
        "\n",
        "# Move model on GPU if available\n",
        "enable_cuda = True\n",
        "device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n",
        "flow_model = flow_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#setup up cifar 100 dataset\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "\n",
        "trainset = NoisyCIFAR10(root='./data', train=True, download=True, noise_std = 0.5, transform=transform)\n",
        "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "trainLoader = DataLoader(trainset, batch_size=256, num_workers=4, shuffle=False)\n",
        "testLoader = DataLoader(testset, batch_size=256, num_workers=4, shuffle=False)"
      ],
      "metadata": {
        "id": "hLJ1VU38sVGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmiOJauDNnP9",
        "outputId": "2975146c-e4fc-4101-c518-2bad1b7ee0b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy = 0.561199963092804\n",
            "0\n",
            "accuracy = 0.6022999882698059\n",
            "1\n",
            "accuracy = 0.6714999675750732\n",
            "2\n",
            "accuracy = 0.7281999588012695\n",
            "3\n",
            "accuracy = 0.7579999566078186\n",
            "4\n",
            "accuracy = 0.7439999580383301\n",
            "5\n",
            "accuracy = 0.7421999573707581\n",
            "6\n",
            "accuracy = 0.7242000102996826\n",
            "7\n",
            "accuracy = 0.7013999819755554\n",
            "8\n",
            "accuracy = 0.6881999969482422\n",
            "9\n",
            "accuracy = 0.7674999833106995\n",
            "10\n",
            "accuracy = 0.7860999703407288\n",
            "11\n",
            "accuracy = 0.7913999557495117\n",
            "12\n",
            "accuracy = 0.7347999811172485\n",
            "13\n",
            "accuracy = 0.7724999785423279\n",
            "14\n",
            "accuracy = 0.78329998254776\n",
            "15\n",
            "accuracy = 0.7335000038146973\n",
            "16\n",
            "accuracy = 0.7764999866485596\n",
            "17\n",
            "accuracy = 0.7859999537467957\n",
            "18\n",
            "accuracy = 0.7976999878883362\n",
            "19\n",
            "accuracy = 0.7644000053405762\n",
            "20\n",
            "accuracy = 0.7785999774932861\n",
            "21\n",
            "accuracy = 0.7953999638557434\n",
            "22\n",
            "accuracy = 0.7935000061988831\n",
            "23\n",
            "accuracy = 0.7719999551773071\n",
            "24\n",
            "accuracy = 0.7953000068664551\n",
            "25\n",
            "accuracy = 0.8108999729156494\n",
            "26\n",
            "accuracy = 0.7906000018119812\n",
            "27\n",
            "accuracy = 0.7946999669075012\n",
            "28\n",
            "accuracy = 0.7864999771118164\n",
            "29\n",
            "accuracy = 0.8133999705314636\n",
            "30\n",
            "accuracy = 0.7907999753952026\n",
            "31\n",
            "accuracy = 0.8121999502182007\n",
            "32\n",
            "accuracy = 0.8093000054359436\n",
            "33\n",
            "accuracy = 0.7971999645233154\n",
            "34\n",
            "accuracy = 0.8184999823570251\n",
            "35\n",
            "accuracy = 0.7627999782562256\n",
            "36\n",
            "accuracy = 0.7875999808311462\n",
            "37\n",
            "accuracy = 0.8159999847412109\n",
            "38\n",
            "accuracy = 0.8000999689102173\n",
            "39\n",
            "accuracy = 0.7857999801635742\n",
            "40\n",
            "accuracy = 0.7827000021934509\n",
            "41\n",
            "accuracy = 0.8134999871253967\n",
            "42\n",
            "accuracy = 0.8167999982833862\n",
            "43\n",
            "accuracy = 0.8047999739646912\n",
            "44\n",
            "accuracy = 0.8127999901771545\n",
            "45\n",
            "accuracy = 0.8197000026702881\n",
            "46\n",
            "accuracy = 0.8097999691963196\n",
            "47\n",
            "accuracy = 0.8127999901771545\n",
            "48\n",
            "accuracy = 0.8179999589920044\n",
            "49\n"
          ]
        }
      ],
      "source": [
        "#intilaize model and run trianig loop, stroe probablities at eahc epoch for teh dataset\n",
        "\n",
        "#data structure for trainng probabilties\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ResNet9(3, 10, 32).to(device)\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "epoch = 50\n",
        "\n",
        "\n",
        "probabilties = [[] for i in range(len(trainset))]\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(epoch):\n",
        "    idx = 0\n",
        "    for batch_idx, (inputs, labels) in enumerate(trainLoader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        model.train()\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track probabilities using absolute indices\n",
        "        start_idx = batch_idx * trainLoader.batch_size\n",
        "        for i, output in enumerate(outputs.detach().cpu().numpy()):\n",
        "            probabilties[start_idx + i].append(output)  # Matches dataset order\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for inputs, labels in testLoader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predictions = torch.argmax(outputs, dim=1)\n",
        "        correct += torch.sum((predictions == labels))\n",
        "    print(f'accuracy = {correct/len(testset)}')\n",
        "\n",
        "    print(epoch)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YIH6jG9M02i",
        "outputId": "64d45771-3b46-4374-8161-2d2d48ff0565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n"
          ]
        }
      ],
      "source": [
        "uncertainties = compute_dyn_unc(probabilties, 10, 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDuArT4sNAFd",
        "outputId": "8a6632de-29e2-49d6-bfd6-6e9b2fa585f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "#load flow model\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 1. Instantiate your NICE model\n",
        "# Construct flow model with the multiscale architecture\n",
        "# 2. Load the state dictionary into the model\n",
        "state_dict = torch.load('/content/drive/MyDrive/flowModels/GLOW_CIFAR10_Clean.pth', map_location=torch.device('cuda'))\n",
        "flow_model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Q-poL10NCjl"
      },
      "outputs": [],
      "source": [
        "#generate log probabilites of each dta point:\n",
        "\n",
        "probabilities = []\n",
        "\n",
        "max_float32 = np.finfo(np.float32).max\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "with torch.no_grad():\n",
        "    for image, targets in trainLoader:\n",
        "        image = image.to(device)\n",
        "        log_prob = flow_model.log_prob(image, targets.to(device))\n",
        "        prob = torch.exp(log_prob)\n",
        "        print(targets)\n",
        "        for i in range(len(image)):\n",
        "            print(log_prob[i].item())\n",
        "            probabilities.append(log_prob[i].item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG5ve9IgNTKd",
        "outputId": "63ba4806-9e21-4c4b-a1e9-ac7d88f3616e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size: 0.5\n",
            "ratio 1\n",
            "max accuracy: 0.8112999796867371\n",
            "max accuracy: 0.8100999593734741\n",
            "size: 0.5\n",
            "ratio 0.5\n",
            "max accuracy: 0.8129000067710876\n",
            "max accuracy: 0.8126999735832214\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "size: 0.25\n",
            "ratio 1\n",
            "max accuracy: 0.7356999516487122\n",
            "max accuracy: 0.7414999604225159\n",
            "size: 0.25\n",
            "ratio 0.5\n",
            "max accuracy: 0.7572000026702881\n",
            "max accuracy: 0.7526999711990356\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ],
      "source": [
        "#train model on pruned dataset\n",
        "\n",
        "ratios = [1, 0.5]\n",
        "\n",
        "sizes = [0.5, 0.25]\n",
        "\n",
        "for k in sizes:\n",
        "\n",
        "    for i in ratios:\n",
        "\n",
        "        print(f'size: {k}')\n",
        "\n",
        "\n",
        "        print(f'ratio {i}')\n",
        "\n",
        "        accuracies = [0.0] * 9\n",
        "        ratio_idx = 0\n",
        "        for j in range(2):\n",
        "\n",
        "\n",
        "            pruned_dataset = prune_dataset(trainset, k, model, uncertainties, probabilities, i)\n",
        "\n",
        "            prune_trainLoader = DataLoader(pruned_dataset, batch_size=256, num_workers=4, shuffle=False)\n",
        "\n",
        "            pruned_model = ResNet9(3, 10, 32).to(device)\n",
        "\n",
        "            pruned_model.to(device)\n",
        "\n",
        "            epoch = 50\n",
        "\n",
        "            optimizer = optim.Adam(pruned_model.parameters(), lr=0.001)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "            max_accuracy = 0\n",
        "\n",
        "            for epoch in range(epoch):\n",
        "                idx = 0\n",
        "                for batch_idx, (inputs, labels) in enumerate(prune_trainLoader):\n",
        "                    pruned_model.train()\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                    # Forward pass\n",
        "                    outputs = pruned_model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Backward pass\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                pruned_model.eval()\n",
        "                correct = 0\n",
        "                for inputs, labels in testLoader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = pruned_model(inputs)\n",
        "                    predictions = torch.argmax(outputs, dim=1)\n",
        "                    correct += torch.sum((predictions == labels))\n",
        "                if correct/len(testset) > max_accuracy:\n",
        "                    max_accuracy = correct/len(testset)\n",
        "            print(f'max accuracy: {max_accuracy}')\n",
        "\n",
        "    print(accuracies)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#comparative tests\n"
      ],
      "metadata": {
        "id": "fEglhTk9_Xlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def random_pruning(dataset, prune_fraction):\n",
        "    n = len(dataset)\n",
        "    keep_n = int(n * (1 - prune_fraction))\n",
        "    indices = np.random.choice(n, keep_n, replace=False)\n",
        "    pruned_dataset = [dataset[i] for i in indices]\n",
        "    return pruned_dataset"
      ],
      "metadata": {
        "id": "ABCMMFZS_XVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_prune_set = random_pruning(trainset, 0.5)"
      ],
      "metadata": {
        "id": "yBGsGDq1GbPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def compute_grand_scores(model, dataloader, device):\n",
        "    model.eval()\n",
        "    scores = []\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        inputs.requires_grad = True\n",
        "        outputs = model(inputs)\n",
        "        loss = torch.nn.functional.cross_entropy(outputs, targets, reduction='none')\n",
        "        for i in range(len(inputs)):\n",
        "            model.zero_grad()\n",
        "            loss[i].backward(retain_graph=True)\n",
        "            grad_norm = 0.0\n",
        "            for param in model.parameters():\n",
        "                if param.grad is not None:\n",
        "                    grad_norm += param.grad.norm().item() ** 2\n",
        "            grad_norm = grad_norm ** 0.5\n",
        "            scores.append(grad_norm)\n",
        "    return scores\n",
        "\n",
        "def grand_pruning(dataset, scores, prune_fraction):\n",
        "    n = len(scores)\n",
        "    keep_n = int(n * (1 - prune_fraction))\n",
        "    top_indices = np.argsort(scores)[-keep_n:]\n",
        "    pruned_dataset = [dataset[i] for i in top_indices]\n",
        "    return pruned_dataset"
      ],
      "metadata": {
        "id": "FNINT7dICtxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def loss_monotonicity_pruning(model, dataloader, device, epochs):\n",
        "    # Initialize a history of losses for each sample\n",
        "    loss_history = np.zeros((len(dataloader.dataset), epochs))\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for idx, (inputs, targets) in enumerate(dataloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = torch.nn.functional.cross_entropy(outputs, targets, reduction='none')\n",
        "            loss_history[idx, epoch] = loss.item()\n",
        "    # Count how many times loss increases for each sample\n",
        "    loss_increases = np.sum(np.diff(loss_history, axis=1) > 0, axis=1)\n",
        "    return loss_increases\n",
        "\n",
        "def prune_by_loss_monotonicity(dataset, loss_increases, prune_fraction):\n",
        "    n = len(loss_increases)\n",
        "    keep_n = int(n * (1 - prune_fraction))\n",
        "    top_indices = np.argsort(loss_increases)[-keep_n:]\n",
        "    pruned_dataset = [dataset[i] for i in top_indices]\n",
        "    return pruned_dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "HpeT9_1KGCkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(gradn_prune_set))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lULHgVGBkBAP",
        "outputId": "8953d74b-a543-413a-a22a-3952db088d93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_prune_set = random_pruning(trainset, 0.75)\n",
        "\n",
        "gradn_scores = compute_grand_scores(model, trainLoader, device)\n",
        "\n",
        "gradn_prune_set = grand_pruning(trainset, gradn_scores, 0.5)\n",
        "\n",
        "model = ResNet9(3, 10, 32).to(device)\n",
        "\n",
        "\n",
        "pruned_sets = [random_prune_set, gradn_prune_set]\n",
        "\n",
        "print(\"trainign sets computed\")\n",
        "\n",
        "for pruned_set in pruned_sets:\n",
        "\n",
        "  prune_model = ResNet9(3, 10, 32).to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(prune_model.parameters(), lr=0.001)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  epochs = 50\n",
        "\n",
        "\n",
        "\n",
        "  prune_loader = DataLoader(pruned_set, batch_size=256, num_workers=4, shuffle=False)\n",
        "\n",
        "  max_accuracy = 0\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      for batch_idx, (inputs, labels) in enumerate(prune_loader):\n",
        "          prune_model.train()\n",
        "          inputs = inputs.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = prune_model(inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          # Backward pass\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      correct = 0\n",
        "      for inputs, labels in testLoader:\n",
        "          prune_model.eval()\n",
        "          inputs = inputs.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = prune_model(inputs)\n",
        "\n",
        "          predictions = torch.argmax(outputs, dim=1)\n",
        "          correct += torch.sum((predictions == labels))\n",
        "      accuracy = correct / len(testset)\n",
        "      print(f'epoch: {epoch}, accuracy: {accuracy}')\n",
        "      if accuracy > max_accuracy:\n",
        "          max_accuracy = accuracy\n",
        "  print(f'max accuracy: {max_accuracy}')\n",
        "\n",
        "          # Backward pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kKX34nB_bEc",
        "outputId": "a10a93e1-6542-4279-9d44-cf64ff1510dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainign sets computed\n",
            "epoch: 0, accuracy: 0.35899999737739563\n",
            "epoch: 1, accuracy: 0.46069997549057007\n",
            "epoch: 2, accuracy: 0.49859997630119324\n",
            "epoch: 3, accuracy: 0.5593000054359436\n",
            "epoch: 4, accuracy: 0.583899974822998\n",
            "epoch: 5, accuracy: 0.5801999568939209\n",
            "epoch: 6, accuracy: 0.6111999750137329\n",
            "epoch: 7, accuracy: 0.6380999684333801\n",
            "epoch: 8, accuracy: 0.5575000047683716\n",
            "epoch: 9, accuracy: 0.40209999680519104\n",
            "epoch: 10, accuracy: 0.4779999852180481\n",
            "epoch: 11, accuracy: 0.5722000002861023\n",
            "epoch: 12, accuracy: 0.5543000102043152\n",
            "epoch: 13, accuracy: 0.5889999866485596\n",
            "epoch: 14, accuracy: 0.5482999682426453\n",
            "epoch: 15, accuracy: 0.46379998326301575\n",
            "epoch: 16, accuracy: 0.4197999835014343\n",
            "epoch: 17, accuracy: 0.6513999700546265\n",
            "epoch: 18, accuracy: 0.5496000051498413\n",
            "epoch: 19, accuracy: 0.5738999843597412\n",
            "epoch: 20, accuracy: 0.6354999542236328\n",
            "epoch: 21, accuracy: 0.6380999684333801\n",
            "epoch: 22, accuracy: 0.6847000122070312\n",
            "epoch: 23, accuracy: 0.7051999568939209\n",
            "epoch: 24, accuracy: 0.7285000085830688\n",
            "epoch: 25, accuracy: 0.7464999556541443\n",
            "epoch: 26, accuracy: 0.7471999526023865\n",
            "epoch: 27, accuracy: 0.7470999956130981\n",
            "epoch: 28, accuracy: 0.746999979019165\n",
            "epoch: 29, accuracy: 0.746999979019165\n",
            "epoch: 30, accuracy: 0.7465999722480774\n",
            "epoch: 31, accuracy: 0.746399998664856\n",
            "epoch: 32, accuracy: 0.7468000054359436\n",
            "epoch: 33, accuracy: 0.7468000054359436\n",
            "epoch: 34, accuracy: 0.7468000054359436\n",
            "epoch: 35, accuracy: 0.746399998664856\n",
            "epoch: 36, accuracy: 0.7462999820709229\n",
            "epoch: 37, accuracy: 0.746399998664856\n",
            "epoch: 38, accuracy: 0.7457999587059021\n",
            "epoch: 39, accuracy: 0.7455999851226807\n",
            "epoch: 40, accuracy: 0.7454999685287476\n",
            "epoch: 41, accuracy: 0.7455999851226807\n",
            "epoch: 42, accuracy: 0.7453999519348145\n",
            "epoch: 43, accuracy: 0.7455999851226807\n",
            "epoch: 44, accuracy: 0.7454999685287476\n",
            "epoch: 45, accuracy: 0.745199978351593\n",
            "epoch: 46, accuracy: 0.7453999519348145\n",
            "epoch: 47, accuracy: 0.7452999949455261\n",
            "epoch: 48, accuracy: 0.7452999949455261\n",
            "epoch: 49, accuracy: 0.7452999949455261\n",
            "max accuracy: 0.7471999526023865\n",
            "epoch: 0, accuracy: 0.20969998836517334\n",
            "epoch: 1, accuracy: 0.19249999523162842\n",
            "epoch: 2, accuracy: 0.17599999904632568\n",
            "epoch: 3, accuracy: 0.17000000178813934\n",
            "epoch: 4, accuracy: 0.16499999165534973\n",
            "epoch: 5, accuracy: 0.1664000004529953\n",
            "epoch: 6, accuracy: 0.17909999191761017\n",
            "epoch: 7, accuracy: 0.1818999946117401\n",
            "epoch: 8, accuracy: 0.17729999125003815\n",
            "epoch: 9, accuracy: 0.17299999296665192\n",
            "epoch: 10, accuracy: 0.17219999432563782\n",
            "epoch: 11, accuracy: 0.16869999468326569\n",
            "epoch: 12, accuracy: 0.17260000109672546\n",
            "epoch: 13, accuracy: 0.16849999129772186\n",
            "epoch: 14, accuracy: 0.1639999896287918\n",
            "epoch: 15, accuracy: 0.1784999966621399\n",
            "epoch: 16, accuracy: 0.1736999899148941\n",
            "epoch: 17, accuracy: 0.19259999692440033\n",
            "epoch: 18, accuracy: 0.17430000007152557\n",
            "epoch: 19, accuracy: 0.17809998989105225\n",
            "epoch: 20, accuracy: 0.1753999888896942\n",
            "epoch: 21, accuracy: 0.17710000276565552\n",
            "epoch: 22, accuracy: 0.18320000171661377\n",
            "epoch: 23, accuracy: 0.19380000233650208\n",
            "epoch: 24, accuracy: 0.1761000007390976\n",
            "epoch: 25, accuracy: 0.17399999499320984\n",
            "epoch: 26, accuracy: 0.1785999983549118\n",
            "epoch: 27, accuracy: 0.17090000212192535\n",
            "epoch: 28, accuracy: 0.18410000205039978\n",
            "epoch: 29, accuracy: 0.18959999084472656\n",
            "epoch: 30, accuracy: 0.1784999966621399\n",
            "epoch: 31, accuracy: 0.1906999945640564\n",
            "epoch: 32, accuracy: 0.19829998910427094\n",
            "epoch: 33, accuracy: 0.19979999959468842\n",
            "epoch: 34, accuracy: 0.18199999630451202\n",
            "epoch: 35, accuracy: 0.1922999918460846\n",
            "epoch: 36, accuracy: 0.19849999248981476\n",
            "epoch: 37, accuracy: 0.19769999384880066\n",
            "epoch: 38, accuracy: 0.18230000138282776\n",
            "epoch: 39, accuracy: 0.18199999630451202\n",
            "epoch: 40, accuracy: 0.16869999468326569\n",
            "epoch: 41, accuracy: 0.18689998984336853\n",
            "epoch: 42, accuracy: 0.17799998819828033\n",
            "epoch: 43, accuracy: 0.19429999589920044\n",
            "epoch: 44, accuracy: 0.19179999828338623\n",
            "epoch: 45, accuracy: 0.17139999568462372\n",
            "epoch: 46, accuracy: 0.18559999763965607\n",
            "epoch: 47, accuracy: 0.17520000040531158\n",
            "epoch: 48, accuracy: 0.18310000002384186\n",
            "epoch: 49, accuracy: 0.1939999908208847\n",
            "max accuracy: 0.20969998836517334\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}